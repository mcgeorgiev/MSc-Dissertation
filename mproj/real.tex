\documentclass{mproj}
\usepackage{graphicx}

\usepackage{url}
\usepackage{fancyvrb}
\usepackage[final]{pdfpages}
\usepackage{times}

% for alternative page numbering use the following package
% and see documentation for commands
%\usepackage{fancyheadings}


% other potentially useful packages
%\uspackage{amssymb,amsmath}
%\usepackage{url}
%\usepackage{fancyvrb}
%\usepackage[final]{pdfpages}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Title of project placed here}
\author{Name of author placed here}
\date{Date of submission placed here}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
abstract goes here
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\educationalconsent

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}

acknowledgements go here sdfsdf

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}\label{intro}

\section{Sentence}

I completed a program which allowed a turtlebot to autonomously map an environment using SLAM as well as create an inventory of items found in the environment.


\section{Importance/Context/Motivation}

Mobile robots are becoming more and more affordable and accessible which has allowed developers to take advantage of their applications in many different ways. 

SLAM has it's application in rescue robots which
http://www.aaai.org/Pressroom/Releases/release-02-0910.php
these robots required realtime control and utilised only video streams to identify and rescue people.

This compares to this which utilises rugged mobile robots to create SLAM maps of mine shafts with minimal supersvision. This can then be applied to areas which are too unsafe/ small for humans to access.
https://miningrox.informatik.tu-freiberg.de/en/

More affordable Drones can also be used to increasingly accurate create maps of property as well

Object detection can be used to detect humans via heat sensors etc. Aswell as identifying bombs etc.



\section{Objectives/Hypothesis Karl Popper/Problem statement}

\section{Description of Objectives}

\section{How I achieved it} 

\subsection{System Diagram} 

\section{Outline of the dissertation} 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}\label{survey}

This chapter will provide research into the hardware and software required to run the turtlebot and perform autonomous SLAM mapping. Currently there is a selection of applications which already perform individual tasks required to achieve this project and thus it is a case of discussing and evaluating each component. The area of object detection and recognition is firstly considered in context of natural visual systems and then in software applications where a discussion of two seminal methods of detection are discussed.

\section{Robot Operating System (ROS)}
\subsection{What is ROS}

ROS is a meta-operating system which provides a collection of tools and conventions to aid the writing of robot software. This includes an abstraction of hardware and communications as well as tools including message passing, package management and low level controllers.

\subsection{Why do people use ROS}

ROS features open-source licenses and a large growing collection of packages which contribute to a vibrant ecosystem of developers and researchers working on applications. Some of these packages includes powerful applications such as the Gazebo simulator and the visualisation tool RViz as well as complex algorithms such as SLAM and drivers such as OpenNI.

ROS encourages software engineering principles of loose coupling and abstraction which simplifies software integration and reuse as well as meaning that ROS is generally independent of hardware specifics. ROS features standard implementations in popular programming languages such as Python and C++ which increases adoption.

\subsection{How does ROS works?}

ROS consists of some key components within the communications infrastructure.
\begin{itemize}
  \item Nodes are modular processes performing computation which can communicate with each other using topics or services. A system may consist of many nodes which perform many different tasks.
  \item Topics are named buses which provide a clear message passing interface between nodes. This is achieved through an anonymous publish/subscribe mechanism. This mechanism allows many-to-many transport.
  \item Messages are a data structure which are published by nodes to topics. They support strongly typed fields such as primitives or arrays and are either predefined or user-defined.
  \item The ROS core or Master provides a centralized node which locates and negotiates communications between nodes as well providing naming and registration services.
  \item Services forgo the many-to-many paradigm and provide a request/reply interaction via servers and clients.

\end{itemize}

\begin{figure}[h]
  \caption{Illustration of the ROS concepts}
  \centering
  \includegraphics[width=0.5\textwidth]{images/ROS_basic_concepts.png}
  \label{fig:ROS diagram}
\end{figure}

For example a simple robot vehicle system can consist of three nodes. The first node controls the robot's ultrasonic range finder and is publishing a stream of range messages taken from the sensor along the \texttt{sensor/sonar} topic.

The second node controls the robot's movement hardware and is publishing the robot's orientation on the \texttt{geometry\char`_msgs/pose} topic. It also contains an actionlib server which responses to requests to change the robot's positioning. 

The third node controls the robot's movement and is subscribed to the \texttt{sensor/sonar} topic and contains a client of the actionlib server. While processing the range messages if the range indicated is very small the node can use the actionlib service to request an adjustment in the orientation of the robot thus meaning the robot will avoid collisions. 

Include diagram describing example


\subsubsection{RViz}

Rviz is a visualisation tool in ROS which provides 3d visualisations of sensor data and robot states, such as cameras, lasers and joint states. Add modular componets which can be customized.

\subsubsection{Gazebo}
Gazebo provides a simulated environment to build and test applications in ROS. A simulated robot will feature all the same sensors as a real one and a model world can be created. Physics, models, programmable features such as doors orlights.


\section{Mobile Robots}


Mobile robots are robots that are not fixed to one location and are capable of moving around their environments. Movement is achieved generally through legs, wheels or tracks but aerial and nautical robots can use propellors. The robot can be controlled through manual-tele operation involving a human driver, line-following which follow visual cues such as painted lines to navigate or autonomously. These robots have many applications in areas such as military, agricultural, rescue, transport and domestic usage and include Unmanned Aerial Vehicles (UAV or commonly drones), rescue robots and self-driving cars.


\subsection{Turtlebot 2}

The Turtlebot 2 is a part of a series of low cost, open-source mobile robots by Clearpath Robotics**. These robots are intended for educational and research purposes and provide an 'low-barrier-of-entry' platform for development. The robot consists of a Kobuki base, a computer running ROS and some form of RGB-D or Stereo Camera such as the Xbox Kinect or Asus Xtion.

Developed by ... for the intended purpose... 

The Kobuki Base contains IR, bump, cliff and wheel drop sensors as well as different power connectors and ports.

The Kobuki Base requires a computer running ROS to communicate with, in most cases this can simply involve a laptop attached to the top of the base connected by a usb cable. However in some cases it may be more convienent to attach a small netbook or Raspberry Pi to the base and use a seperate computer to communicate with the host.

Why use the turtlebot?

- Lots of software standard packages support, gazebo models, teleoperation, slam as well as large and growing community of stuff...
- Abstracted hardware, no need to understand how it works, just recieve the data,

How it works in ros

Odometry

\section{Cameras}
Why do we need them
Why discuss both of these? - exemplar
\subsection{RGB-D}
\begin{itemize}
  \item How it works
  \item Active camera - range finder
  \item Kinect specs
\end{itemize}
\subsection{Stereo}

\begin{itemize}
  \item Passive camera, works well outdoors
  \item How depth is calculated
  \item How images are combined 
  \item About the zed cam - specs
\end{itemize}

\section{SLAM}
\begin{itemize}

  \item What is the problem, include references e.g. to correlation
  \item explain the proble
  \item solutions to the problem

 
\end{itemize}


SLAM is the problem of simultaneously localising (finding the pose and orientation) of a camera within it's surroundings at the same time as mapping the structure of the environment. This requires a robot or camera with the ability to produce odometry readings as well as camera with a range measurement device, either a range finder or found in a stereo camera. 

SLAM forms the basis of navigation in most mobile robots, meaning unknown environments can be explored and mapped without the need for technology such as GPS which becomes innaccurate within two metres or in interiors. It has applications in a range of manned and autonomous robots. This includes UAV's, underwater robots and domestic robots such automatic lawnmowers. SLAM is a key component in the development of self-driving cars These cars which are driven along routes while performing SLAM capturing location, feature and obstacle data. Once the map is completed it is processed and the cars are driven autonomously along these routes updating the map as necessary.



This is considered to be a solved problem in Computer Science and their are many different approaches for finding a solution. \cite{Hugh2006}



During the development of the SLAM problem researchers established the correlation between estimates of a robots location and the landmarks which consituate the map







Building maps once can work, but it can be distrupted by changes in the environment, e.g lighting 

We know the position of the landmarks
Localisation requires landmarks, estimate the location of these and estimate it's own pose relative to it. This can be inaccurate due to odometry or inaccurate readings, rely on each. 95% --> give gaussian estimate
GEt estimate of where the robot probably is, get it's location relative to landmarks

Cannot fully decouple localisation and mapping as map is needed for localisation and a pose estimate is needed for mapping

Areas which are missing. Passive.

Given: Robot's controls, Observations(laser scans)
Wanted: map of environment
Path of the robot.

Probablistic techniques, uncertainity of robots motions and observations... represent uncertnity typically in a Gaussian distribution, error can accumalate over time. Take the uncertanity into account.

Slam is difficult because both path and map are unknown, but they are correlated and depend on each other





On a high level SLAM is solved by using the environment to update the pose of the robot. Using odometry as the sole measurement of localisation has an element of uncertainity due to extraneous factors such as wheels slipping on different environments meaning that a stated distance given by an odometry reading may be over or under estimated. Therefore laser scans or other forms of depth readings are used to correct the robot's position by extracting features from the surrounding environment. These are called landmarks and can be extracted by various methods such as Random Sampling Consensus (RANSAC) and provide a growing map of the enviroment. 


The robot estimates the location of it's own pose relative to these different landmarks which are all correlated together and increase with successive observations. \cite{Hugh1988}
  
The combined localisation and mapping estimates as a single estimation problem are convergent  
  
  Extended Kalman Filter is used to update where the robot presumes it is based on these landmarks and keeps track of the uncertainity of the robots position and the landmarks seen.
  
  When robot moves the uncertainity of the new position is updated.
  Landmarks are extracted and reobserved ones are used to update the robots position. New landmarks are added to the EKF. 
  
  ***Diagram. of sequence of reaffirmation.


Uncertanity is kept track of.



\subsection{RtabMap}

- intro RTAB-Map is a RGB-D graph based SLAM approach. What it is

- Loop closure technique. How it works

Online slam, seeks to recover only the most recent pose, not the entire path algorithm

Localisation of a known map can be achieved with similar to orientation flight on animals insects -

\subsection{Others}

Compare rtabmap and other papers Cartographer

\section{Frontier Exploration}






\section{Object Detection and Classification}

\subsection{Introduction}
Object detection is the task of finding the different objects in an image and classifying them 

Within Computer Science object detection refers to finding real-world objects within images or videos. Alongside robotics, such technology has applications in areas such as surveillance, medical image analysis and human computer interaction. It is useful to think of object detection software in terms of an intelligent agent which is an autonomous entity that gathers and observers information through sensors and acts upon an environment using actuators to direct its activity to achieve goals.\cite{Norvig2003} Why?

Understood in natural world with particular emphasis on lower organisms.

\subsection{Detection in the natural world}

It is easy to assume that an eye is comparable to a camera in that it streams data through the lense. Many organism's vision systems have much less resolution, sensitivity and field of vision than a modern camera yet are able to perform incredibly complex tasks successfully. 

Modelling a intelligent agent that interprets objects in scenes requires an understanding of natural visual systems. For most systems while everything is seen for the first time our senses do not keep telling us things we already know, this is because between scenes most important environmental information stays constant. This process is a form of sensory adaption where perception is temporarily changed when exposed to new stimuli in an attempt to normalise visual experience.\cite{Webster2015} This happens at a retinal and neural level, where information provided by past experience have a greater say on how a scene has been interpreted than immediate information provided by external organs. 

Contrast human vs lower

For example an ellipse projected into a human retina can be interpreted as a circle due to our experiences with perspective. 
%\cite{The eye the brain the computer p208}

Many species exhibit a form of pattern or feature detection to trigger neural responses to visual changes. In contrast to humans which detect generic images features such as shapes many lower organisms utilise goal-based feature detection, this is useful to the field of computer science because it allows us to model intelligence agents on such processes and tend to engage in one activity at a time using only one way of doing it \cite{} while achieving complex goals and tasks.pg 17 easier to model intelligence agents.

For example arthopods such as honey bees readily distinguish features such as flowers in the environment that pertain to the goal of gathering food. Similarly a frog's eye is stimulated when a black disc moves in an arc rapidly within the receptor field indicating that a flying insect is near and triggers the frogs feeding response \cite{} thus satisfying the goal of feeding.

It is important to note that these examples all feature a form of active vision whereby the  eye will "actively acquire the visual cues needed to cope with a particular task" pg 17  in the case of a human the fovea will align with the area of interest in the environment, comparable to something like a pan and tilt surveillance system which focus on areas of interest.
 
 
\subsection{Extracting Features}

What is a feature? Somewhere!

In contrast to higher organisms Lettvin et al. \cite{} determined that a frog's retina has four  classes of ganglion cells which specialize in extracting features from a visual signal. 
\begin{itemize}
  \item edge detectors for borders between light and dark areas
  \item moving edge detectors
  \item dimming detectors 
  \item convex detectors that react when a black disc is in the field of vision
\end{itemize}

Image processing software has a similar definition of feature of extraction but in most cases they are generic. Explain

\subsubsection{FAST Feature Detection}

There are many different software applications of edge and corner detection, this includes SIFT (Scale-Invariant Feature Transform) which performs extremely well in most contexts. \cite{Mikolajczyk} However for applications in real-time image processing such as SLAM and Augmented Reality it is often not fast enough. FAST (Features from Accelerated Segment Test) \cite{rosten_2006_machine}\cite{rosten_2005_annotations} avoids the costly difference of Gaussians (DoG) method found in SIFT and is subsequently considerably faster.

Using an appropriate threshold T which is usually 12, the algorithm selects a pixel P which is the centre of a circle with a circumference of 16 pixels n and the pixel's p intensity is Ip. The pixel P is determined to be a corner if n are all brighter that  Ip + t or are all darker than Ip - t. 

\begin{figure}[h]
  \caption{Illustration of the FAST}
  \centering
  \includegraphics[width=0.5\textwidth]{images/fast_speedtest.jpg}
  \label{fig:FAST diagram}
\end{figure}

If a threshold T of 12 or greater is used a high speed test is performed to reject a large number of non-corner points which involves examining the pixels at 1, 9, 5, 13. If either pixel 1 or 9 are brighter or darker than T then 5 and 13 are checked. If three of these pixels are either all darker than the threshold or all brighter than p is a corner. 

\subsubsection{software appliations involving fast}

\subsection{Object Classification}

However in contrast to simple organisms whose focus is on detection resulting in a sensitive and bespoke! and broad field of vision the programme must have the ability to classify which requires precise resolution when required.

Not feasible to search for objects at all locations in a visual field
Partioning or perceptual organisation.

Law of proximity = Stimulus that are close together are percieved to be a group.

Geometric, Photometric modeling scene degmentation, naming+labeling,

Image matching:
correlation approach
 
feature matching approach
eges remain across two images of the same scene. 
relational matching approach

From page 281 --> talk about image labeling

\subsubsection{Deep Leaning}

  \subsubsection{Tensorflow}
  \subsubsection{Haar Cascades}
  \subsubsection{Google Vision API}

\subsection{Regions with Convolutional Neural Networks (R-CNN)}

Detection systems utilise classifiers to evaluate potential objects. Yet there is know standard way of localising objects within an image. One such approach is called Regions with Convolutional Neural Networks (R-CNN) and is considerable more efficient than previous approaches and at the time of it's release, R-CNN had the best detection performance on the PASCAL VOC 2012 image dataset. In the paper "Rich feature hierarchies for accurate object detection and semantic segmentation"\cite{Girshick2014} the authors suggest a method which involves taking an image and identifying objects using bounding boxes and then performing a classification on these areas to create a label for each object. 

\begin{figure}[h]
  \caption{Illustration of the process stages found in R-CNN}
  \centering
  \includegraphics[width=0.5\textwidth]{images/RCNN.png}
  \label{fig:RCNN Diagram}
\end{figure}

Roughly two thousand bounding boxes, or region proposals are created using the process of Selective Search which performs a segmentation algorithm that groups regions together by color, intensity or texture.\cite{Sande2013} This is in contrast to using an exhaustive sliding window approach found in Deformable Parts Models.\cite{voc-release4} Each selected region is warped to a 227 x 227 square RGB image and fed through a convulutional neural network (CNN) which computes features. The final stage involves running a Support Vector Machine (SVM) on the feature vector of each region classify and score the object within the region (if any). Greedy non-maximum suppression is applied to merge the regions which share the same object resulting in accurate bounding boxes for each object. 

This method is slow as it requires the CNN to be run on every region created for the image.




\subsection{Attention Systems - Memory Hierachy}
\subsubsection{Spatial temporal Attention Models}
Neurophysiologic studies have shown that, in humans, these two 
factors are the main responsible ones to drive attention. Bottom-up factors emanate from the scene and 
focus attention on regions whose features are sufficiently discriminative with respect to the features 
of their surroundings. On the other hand, top-down factors are derived from cognitive issues, such as 
knowledge about the current task.
http://www.prip.tuwien.ac.at/people/krw/more/papers/2012/Antunez2012a.pdf

How long do organism retain visual information.

How spatial temporal reasoning
- don't overload memory
- Have a Buffer
- reaffirm spatial temporal Attention
Memory Hierarchy
- 9 seconds, How long?
- What is long term?
- What is short term? 
- What is important
- How to decide
- Intelligence agents



\subsection{Summary and Discussion}
\subsubsection{What currently exists for completing the objectives}
\subsubsection{What are the Gaps, What do I have to do to fill the gaps}
\subsubsection{What methods are not appropriate to fill the gap}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Approach/Implementation/System Implementation}

\section{Requirements}


\section{Anaylsis the approaches}

What problems I encounted etc.

\section{Analysis of approaches}
\section{How did I go about it}

\section{Mapping}
\subsection{Transforming data}
\subsection{Calibration}

\section{Frontier Exploration}
Hybrid approach taken from RCNN, currently only has an official matlab binding of RCNN.

Do not use bounding boxes, as not entirely useful in mapping as this requires new detections and classifiations when changing the object pose. A simple sspherical location detector is used to show the object is located within the this region of alpha

\section{Object Detection and Recognition}
\subsection{Detecting ROI in FOV}
\subsubsection{Masking}
\subsubsection{Depth Mask}
\subsubsection{HSV Mask}
\subsection{Detecting Objects Clustering - different methods}
\subsection{Creating Boxes with DBScan}
\subsection{Tracking Boxes}
\subsection{Recognising Objects}

Using google vision means that speed does not decreae with more objects detected, statys constant

\subsection{Publishing to Rviz/Rtabmap}
\section{The whole package - how to utilise}

I did this because of that which allows me to do this

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
\section{Testing}
https://www.koen.me/research/pub/vandesande-iccv2011.pdf

Experiment 1
Question
Experiment 2
Experiment 3


I think by doing this I will achieve...
Experiments to answer questions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}\label{conclusion}
\subsection{Future work}

incorporate active detection of features next time.

\appendix % first appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{First appendix}

\section{Section of first appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Second appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% it is fine to change the bibliography style if you want
\bibliographystyle{plain}
\bibliography{mproj}
\end{document}
